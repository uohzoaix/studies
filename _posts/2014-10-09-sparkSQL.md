---
layout: post
title: "spark之路第十课——spark SQL编程"
description: "spark学习系列第十篇"
category: 
- spark
tags: []
---


spark SQL支持像SQL，hiveQL或scala那样的关系型查询语句，sparkSQL中核心的组件是SchemaRDD，SchemaRDD由row对象和描述每行中每列的数据类型的schema组成。SchemaRDD和关系型数据库中的表类似，可以从已存在的RDD，parquet文件，json数据集或运行HiveQL创建而来。  
sparkSQL中最重要的一个类是SQLContext或它的继承类，创建SQLContext如下：
{% highlight objc %}
val sc: SparkContext // An existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
// createSchemaRDD is used to implicitly convert an RDD to a SchemaRDD.
import sqlContext.createSchemaRDD
{% endhighlight %}
除了基本的SQLContext外，还可以创建HiveContext，它提供了SQLContext所没有的超函数，其他的特征包括使用更复杂的HiveQL解析器进行解析查询语句、访问HiveUDF、从hive表中读取数据。当使用HiveContext，不必安装hive，并且任何SQLContext可用的数据源都可用于HiveContext。  
指定的用于解析查询语句的SQL变体可通过spark.sql.dialect选项进行设置，该参数可通过SQLContext的setConf方法或在SQL中使用SET key=value进行设置。对于SQLContext来说，唯一可用的dialect是sql，它使用了SparkSQL提供的一个简单的SQL解析器，对于HiveContext来说，默认的dialect是hiveql，sql也是可用的，但HiveQL解析器会复杂的多。
###数据源
sparkSQL支持多种数据源，SchemaRDD可被当作普通的RDD操作并能被注册为一个临时表，注册SchemaRDD成为一个表允许你在它的数据上运行SQL查询。本节描述了多种将数据导入SchemaRDD的方法。
####RDD
sparkSQL支持两种不同的方式来将已存在的RDD转换为SchemaRDD，第一种方式使用反射来推断RDD的schema，这种方式会有很多简洁的代码并且能在你书写spark应用程序时已经知道schema的情况下很好的工作。
#####使用反射推断schema
sparkSQL中的scala接口支持自动将包含case类的RDD转换为SchemaRDD，其中case类定义了表的schema。case类中的属性会被用于表中的列名，case类还可以被嵌套或包含复杂类型如Sequence或Array。
{% highlight objc %}
// sc is an existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
// createSchemaRDD is used to implicitly convert an RDD to a SchemaRDD.
import sqlContext.createSchemaRDD
// Define the schema using a case class.
// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,
// you can use custom classes that implement the Product interface.
case class Person(name: String, age: Int)
// Create an RDD of Person objects and register it as a table.
val people = sc.textFile("examples/src/main/resources/people.txt").map(_.split(",")).map(p => Person(p(0), p(1).trim.toInt))
people.registerTempTable("people")
// SQL statements can be run by using the sql methods provided by sqlContext.
val teenagers = sqlContext.sql("SELECT name FROM people WHERE age >= 13 AND age <= 19")
// The results of SQL queries are SchemaRDDs and support all the normal RDD operations.
// The columns of a row in the result can be accessed by ordinal.
teenagers.map(t => "Name: " + t(0)).collect().foreach(println)
{% endhighlight %}
#####编程方式指定的schema
当case类不能被提前定义（如数据被编码成字符串或文本数据集将被解析），这时可以以编程方式来创建SchemaRDD：  
1.从原始的RDD创建行的RDD  
2.创建匹配第一步创建的RDD中的行结构的StructType  
3.使用SQLContext的applySchema方法将schema应用到行RDD中
{% highlight objc %}
// sc is an existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
// Create an RDD
val people = sc.textFile("examples/src/main/resources/people.txt")
// The schema is encoded in a string
val schemaString = "name age"
// Import Spark SQL data types and Row.
import org.apache.spark.sql._
// Generate the schema based on the string of schema
val schema =
  StructType(
    schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, true)))
// Convert records of the RDD (people) to Rows.
val rowRDD = people.map(_.split(",")).map(p => Row(p(0), p(1).trim))
// Apply the schema to the RDD.
val peopleSchemaRDD = sqlContext.applySchema(rowRDD, schema)
// Register the SchemaRDD as a table.
peopleSchemaRDD.registerTempTable("people")
// SQL statements can be run by using the sql methods provided by sqlContext.
val results = sqlContext.sql("SELECT name FROM people")
// The results of SQL queries are SchemaRDDs and support all the normal RDD operations.
// The columns of a row in the result can be accessed by ordinal.
results.map(t => "Name: " + t(0)).collect().foreach(println)
{% endhighlight %}
####parquet文件
parquet是柱状格式数据，它可以被其他很多的数据处理系统支持，sparkSQL支持读写parquet文件以自动保护原始数据的schema。
#####编程方式导入数据
例子如下：
{% highlight objc %}
// sqlContext from the previous example is used in this example.
// createSchemaRDD is used to implicitly convert an RDD to a SchemaRDD.
import sqlContext.createSchemaRDD
val people: RDD[Person] = ... // An RDD of case class objects, from the previous example.
// The RDD is implicitly converted to a SchemaRDD by createSchemaRDD, allowing it to be stored using Parquet.
people.saveAsParquetFile("people.parquet")
// Read in the parquet file created above.  Parquet files are self-describing so the schema is preserved.
// The result of loading a Parquet file is also a SchemaRDD.
val parquetFile = sqlContext.parquetFile("people.parquet")
//Parquet files can also be registered as tables and then used in SQL statements.
parquetFile.registerTempTable("parquetFile")
val teenagers = sqlContext.sql("SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19")
teenagers.map(t => "Name: " + t(0)).collect().foreach(println)
{% endhighlight %}
#####配置
parquet的配置可通过SQLContext的setConf方法或通过运行SET key=value方式完成。  
<table>
<thead>
<tr class="header">
<th align="left">属性名</th>
<th align="left">默认值</th>
<th align="left">注释</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">spark.sql.parquet.binaryAsString</td>
<td align="left">false</td>
<td align="left">一些其他的支持parquet的系统如impala或旧版本的spark，在写出parquet schema的时候不会区分二进制数据和字符串。该配置选项告诉sparkSQL将字符串解析为二进制数据以使兼容这些系统</td>
</tr>
<tr class="even">
<td align="left">spark.sql.parquet.cacheMetadata</td>
<td align="left">false</td>
<td align="left">打开缓存parquet schema元数据会提升静态数据的查询速度</td>
</tr>
<tr class="odd">
<td align="left">spark.sql.parquet.compression.codec</td>
<td align="left">snappy</td>
<td align="left">设置写parquet文件的压缩方式，包括：uncompressed，snappy，gzip，lzo</td>
</tr>
</tbody>
</table>
####json数据集
sparkSQL可以通过json数据集推断出schema并将该schema转换为SchemaRDD，该转换可以通过SQLContext的以下两个方法实现：
<li>jsonFile-从json文件所在目录中导入数据，文件的每行必须是一个json对象
<li>jsonRdd-从已存在的RDD导入数据，RDD中的每个元素是包含json对象的字符串
{% highlight objc %}
// sc is an existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
// A JSON dataset is pointed to by path.
// The path can be either a single text file or a directory storing text files.
val path = "examples/src/main/resources/people.json"
// Create a SchemaRDD from the file(s) pointed to by path
val people = sqlContext.jsonFile(path)
// The inferred schema can be visualized using the printSchema() method.
people.printSchema()
// root
//  |-- age: IntegerType
//  |-- name: StringType
// Register this SchemaRDD as a table.
people.registerTempTable("people")
// SQL statements can be run by using the sql methods provided by sqlContext.
val teenagers = sqlContext.sql("SELECT name FROM people WHERE age >= 13 AND age <= 19")
// Alternatively, a SchemaRDD can be created for a JSON dataset represented by
// an RDD[String] storing one JSON object per string.
val anotherPeopleRDD = sc.parallelize(
  """{"name":"Yin","address":{"city":"Columbus","state":"Ohio"}}""" :: Nil)
val anotherPeople = sqlContext.jsonRDD(anotherPeopleRDD)
{% endhighlight %}
####hive表
sparkSQL同时也支持读写存储在hive中数据，然而，由于hive有很多的依赖，所以在使用hive前需要运行sbt/sbt -Phive assembly/assembly来编译一个包含hive的jar包。这个jar包也需要放到所有的worker节点上。  
配置hive的一些参数需要放在conf目录的hive-site.xml文件中。  
当需要使用hive则必须构造一个HiveContext，它继承于SQLContext，当用户没有部署过hive也可以创建HiveContext，当没有配置hive-site.xml文件，那么context会自动在当前目录中创建metastore_db和warehouse。
{% highlight objc %}
// sc is an existing SparkContext.
val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)
sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
sqlContext.sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src")
// Queries are expressed in HiveQL
sqlContext.sql("FROM src SELECT key, value").collect().foreach(println)
{% endhighlight %}
###性能优化
对于很多情况来说需要使用将数据缓存在内存中或使用一些试验性选项来优化的方式来提高性能。
####将数据缓存在内存
