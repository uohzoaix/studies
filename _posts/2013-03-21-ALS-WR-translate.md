---
layout: post
title: "ALS-WR算法论文翻译"
category: 
- translate
- ALS-WR
tags: []
---





经过3个晚上的翻译，终于把ALS-WR算法的介绍论文翻译完成。此次翻译目的是加强对ALS-WR算法的理解和练习自己对专业性英文的能力，由于 本人英文水平有限并且该算法使用到了多个高数甚至超越高数和线性代数的一些知识，所以如哪里翻译不对或理解有误，望英语强人，数学高人，算法牛人给个纠 正，先于此谢过。原文见：http://link.springer.com/chapter /10.1007%2F978-3-540-68880-8_32?LI=true#page-1，最好是看英文版的，因为该算法的主要精髓是在那几个数 学公式上。译文如下：</br></br>
大规模并行协同过滤算法(Netflix Prize)</br></br>
作 者：Yunhong Zhou（http://www.linkedin.com/pub/yunhong-zhou/1/4b9/865）, Dennis Wilkinson, Robert Schreiber and Rong Pan（http://www.linkedin.com/in/rongpan）</br></br>
很多的推荐系统使用协同过滤技术向用户推荐相关事物，该 技术是基于用户之前已经查阅，购买或评论过的事物所做出的推荐。该技术需要解决的两个主要问题是可扩展性和用户信息的稀疏程度带来的问题。在此论文中，我 们将会描述ALS-WR,它是我们为Netflix Prize设计的并行算法。我们在linux集群中使用并行Matlab作为实验平台，根据经验，ALS-WR的性能会伴随着特征值数量和ALS迭代的增 加而增加。ALS-WR使用1000个从RMSE中 获得的特征值作为Netflix数据集进行实验最后获得的得分是0.8985，这个得分是基于单纯（串行）方法获得的最好的得分。结合其他并行版本的方 法，我们获得了比Netflix自身的CineMatch推荐系统高出5.91%的性能。ALS-WR是很简单的并且对于大数据集也有很好的扩展性的一种 解决方法。</br></br>

##1. 介绍</br></br>

推荐系统尝试基于可用的信息向潜在的客户推荐如电影，音乐，网页，产品等事 物。可想而知，一个成功的推荐系统能够明显的提升电子商务公司的收入或促进社交网站上用户的交互。在众多的推荐系统中，基于内容的方法当然是要去分析对应 的内容（如文本，元数据，特征）来决定相似的事物，然而协同过滤对大量用户的行为/爱好向指定用户推荐相似事物。协同过滤在如 Amazon,Netflix,Google News等很多公司中都有使用。</br></br>
Netflix Prize这个项目（据说有大量的奖金给参赛者）是由Netflix主持的大型的数据挖掘竞争项目，主要目的是为了寻找最好的推荐系统算法来预测用户对电 影的评分，当然这是在一个多于1亿得分（根据480000个用户和将近18000部电影得出的得分）的训练集的情况下完成的。每个训练数据有用户，电影， 日期，评分四个元素组成，评分是一个介于1到5的值。测试数据集是由2800000个数据节点和评分组成，目标是在预测评分时将RMSE（均方根误 差，http://wiki.mbalib.com/zh-tw/%E6%A0%87%E5%87%86%E8%AF%AF%E5%B7%AE）的值最小 化。Netflix自身的推荐系统在测试数据集上的得分是0.9514，Netflix Prize这个项目的目标是获得比该得分高出10%的解决方案。</br></br>
该 问题展示了一系列的挑战，（所以到目前为止，奖金还未被参赛者赢取）第一个挑战是数据集比之前的基准数据集还要大100倍，这会消耗大量的模型训练时间和 系统内存要求。第二个挑战是只有1%的用户-电影矩阵是在观察中的，大部分潜在的评分都是不可见的。第三个挑战是由于用户行为训练数据集和测试数据集都会 存在脏数据，我们不能期望用户是完全可预测的。第四个挑战是分配给每个训练和测试数据集中用户的评分是不同的，由于训练数据集是由1995-2005年之 间的数据组成而测试数据集是由2006年的评分组成。特别的，评分越少的用户在测试集中是更为有效的。直观来讲，预测一个在训练集中表示较为稀疏（该用户 对事物的评分较少）的用户的评分是一件很困难的事情。</br></br>
在此论文中，我们将会详细讨论该问题，接着会描述一个并行算法：ALSWR。</br></br>
 
##2.问题结构（配方）</br></br>
使R={rij}nu×nm（数学公式请参看原文）表示一个用户-电影矩阵，其中rij表示i用户对j电影的评分，这个分数可以是一个真实的数字或是无值的。nu指定了用户数量，nm指定了电影数量。在许多的推荐系统中，其任务就是根据已知的评分值去估算那些未知的评分。</br></br>
我 们从矩阵R的一个低秩近似值开始讲起。通过赋予用户和电影一个小尺寸的特征空间我们可以接近用户和电影的数据模型。每个用户电影都存在一个特征,并且每个 用户对电影作出的评分(已知和未知的)对应着用户和电影特征向量值的一个模型。更为特别的是，U=[ui]代表用户的特征矩阵，ui属于 Rnf,其中i=1…nu，M=[mj]表示电影特征矩阵，mj属于Rnf其中j=1…nm，这里的nf是特征空间的尺寸，也就是模型中隐藏变量的数量。如果用户评分是完全可以预测的并且nf也足够的大，我们可能会期望rij = < ui,mj >, 8 i, j。然后在实验当中，我们最小化U和M的一个损失函数（http://wiki.mbalib.com/zh-tw/%E6%8D%9F%E5%A4 %B1%E5%87%BD%E6%95%B0）来获得矩阵U和M。在本文中，我们将会学习mean-square(http://en.wikipedia.org/wiki/Root_mean_square)这个损失函数，该损失是由于一个单一评分是被squared error(http://en.wikipedia.org/wiki/Mean_squared_error)定义的：</br>
L2(r, u,m) = (r− < u,m >)2.。</br></br>
接下来我们有以下的经验，将所有的损失作为已知评分的损失的和：</br>
Lemp(R,U,M) =1/n X(i,j)2IL2(rij , ui,mj),其中i代表已知评分集合的索引，n代表I的大小。最后可以将低秩近似值问题定义为：(U,M) = arg min(U,M)Lemp(R,U,M)。其中U和M是真实的矩阵，并有nf列。在这个问题中，等式3中有(nu+nm)*nf个参数将被决定，另一方 面，已知的评分集合I的元素个数远远少于nu*nm，因为对于很少的用户来对18000个电影进行评分是不可能的。为了解决此问题，对一个很稀疏的数据集 使用等式3经常会过于庞大，为了避免这个问题，一个简单的方法就是使用一下方式：Lreg_ (R,U,M) = Lemp(R,U,M) + _(kUUk2 + kMMk2)。对于一系列的已选的Tikhonov(http://en.wikipedia.org/wiki/Tikhonov_regularization)矩阵。</br></br>
 
##3.解决方法</br></br>
在此节中，我们描述了一个迭代算法（ALSWR）来解决低秩近似值问题，接着会实现基于并行Matlab平台的并行实现。</br></br>
###3.1ALSWR</br></br>
由于评分矩阵同时包含信号和脏数据，重要的是移除脏数据和使用恢复的信号来预测失去的评分。奇异值分解 (SVD，http://zh.wikipedia.org/zh-cn/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86 %E8%A7%A3)是一种估算原生用户-电影评分矩阵R的方法，它使用两个k排名的矩阵˜R = UT ×M的结果作为估算标准。由SVD给出的解决方案最小化Frobenious norm(http://mathworld.wolfram.com/FrobeniusNorm.html)，该方案与在R的所有元素上最小化RMSE是等效的。然而，由于在矩阵R中有很多的空值，标准的SVD算法不能找到U和M矩阵。在本文中，我们使用ALS来解决低秩近似矩阵分解问题的步骤如下：</br></br>

  1. 使用指定电影的平均得分作为矩阵M的第一行，余下的行值使用小的随机值来填充。</br>
  2. 使用squared errors和的最小值来填充U矩阵。</br>
  3. 相似的使用squared errors和的最小值来填充M矩阵。</br>
  4. 重复2,3步直到满足了停止标准（stopping criterion（http://www.netlib.org/utk/papers/templates/node83.html））。</br></br>

这 里使用的停止标准是基于在探测数据集上观测的RMSE，修改U和M的第一次循环之后，如果探测数据集上的观测的RMSE之间的差异小于1个基点，那么该循 环就会停止并且会使用获得的U,M来作为测试数据集上的最后预测根据。该探测数据集是由Netflix提供的，它与那些隐藏的测试数据集有相同的结构。</br></br>
之 前的第二节中，存在许多的自由参数，在没有正交化的情况下，ALS可能会导致过度拟合（http://en.wikipedia.org/wiki /Overfitting），一种常见的解决方法是使用吉洪诺夫正交化（Tikhonov regularization（http://en.wikipedia.org/wiki/Tikhonov_regularization）），该方 法会使大量的参数尽量影响最小化。我们尝试了很多的正交化矩阵，发现下面的WR正交矩阵的效率最高，因为根据经验当我们增加特征值的数量或循环次数的时候 它从不会导致测试数据过度拟合：f(U,M) = X(i,j)2I(rij − uTi mj)2 + _0@Xinuikuik2 +Xjnmj kmjk21A（公式看原文！！！），其中nui和nmj分别表示用户i和电影j的评分数量，如果使Ii代表用户i评论过的电影j的集合，那么nui就是 Ii的基数，相似的如果Ij表示对电影j评论过的用户集合，那么nmj就是Ij的基数。这个和吉洪诺夫正交化中的U = diag(nui)和M = diag(nmj )是一样的。</br></br>
现在我们将演示如何在M给定的情况下填充（solve）矩阵U。U的列数（ui）是在已知用户i的评分和用户i评论过的电影特征向量关系下解决正规化的线性最小二乘问题时给定的。（公式省略见原文）。</br></br>
相似的，当M被修改时，我们会通过正规化线性最小二乘解来计算mj的值，方法是使用用户对电影j评分特征向量和对电影j的评分，如下：mj = A−1j Vj , 8j。</br></br>
###3.2使用加权正规化来并行计算ALS</br></br>
我 们通过并行修改U和M来并行计算ALS，我们使用的是最新版本的Matlab，该版本的Matlab允许在分开的Matlab上进行并行计算并互相通信 （如分布式数据库），这些分开的Matlab拥有自己的空间和硬件环境。每个这样的Matlab被称为一个单独的实验室，所以我们就会通过识别码 （labindex）和一个静态变量（numlabs）得知当前共有多少个labs。这些矩阵可以是私有（拥有自己的一个拷贝并且值都是不同的）的，也可 以是复制过来的（私有但是所有的labs的值都是相同的）或是分布式（只有一个矩阵但在所有的labs当中是分区存储的）的。分布式的矩阵是存储和使用大 容量数据集的简单方式，因为数据集过大的话，在一个存储器上存储是非常耗内存的。在我们的问题上，我们会使用评分矩阵R的拷贝作为分布式来存储，一份只存 储行（例如只有用户）另一份只存储列（只有电影）。我们将会计算分布式的值并修改U和M，在计算U时我们会使用一份M的拷贝，反之亦然。</br></br>
为 了修改M，我们需要U的一份拷贝，我们会使用电影的评分数据来修改。这是由相同数量的电影分配的。存储电影j评分的lab自然地会成为M的列值，这些列值 也被称为影片j的特征向量。每个lab并行地在对应的影片组中为所有的电影计算mj值，这些计算出来的值接着会被收集起来以保证在一个复制的数组当中每个 节点都有M的所有数据。相同的要修改U，所有的用户会被分成相同数量的用户组，接下来每个lab用以行分开的评分值来在对应用户组中修改用户向量。下面的 Matlab代码片段实现了给定U情况下修改M的实现：</br></br>
{% highlight objc %}
function M = updateM(lAcols, U)
lamI = lambda * eye(Nf);
lM = zeros(Nf,Nlm); lM = single(lM);
for m = 1:Nlm
users = find(lAcols(:,m));
Um = U(:, users);
vector = Um * full(lAcols(users, m));
matrix = Um * Um’ + locWtM(m) * lamI;
X = matrix \ vector;
lM(:, m) = X;
end
M = gather(darray(lM));
End
{% endhighlight %}
对于以上的Matlab代码，lAcols是一份R中列值的本地拷贝，locWtM是所有电影在分开的电影组中的nmj向量，Nlm表示在电影组中的电影数量。Nf和lambda对应着nf和λ，它们是ALS-WR中唯一可变的参数。</br></br>
由 于使用了分布式（不共享内存算法）方式，该方法的计算时间节省了5%。该算法实现了一个接近线性加速的功能：如果nf=100,那么在使用一个处理器的情 况下需要花费2.5小时来修改U和M，但如果有30个处理器那么只需要5分钟。在30个循环的条件下，计算100个隐藏因子需要2.5个小时。</br></br>
 
##4.Netflix Prize问题性能研究</br></br>
将 我们的实验在HP Proliant DL380 G4上的30个处理器集群上运行，所有的处理器是Xeon 2.8Hz并且每四个处理器分享6GB的内存。对于每个nf值，我们将ALS-WR进行了10到25此的循环，并且在修改U和M时如果在探测数据及上增加 了少于1bps的RMSE分数时循环就会轻质。最佳的λ值会在试验和错误的情况下产生，测试的RMSE集合是从Netflix Prize网站的子任务获得的，该测试分数的真正之对我们来说是未知的，为了创建模型和调整参数，我们将探测数据集从训练数据集中排除并将它用于测试。探 测数据集是训练数据集的一个子集，是由Netflix提供的，它包含了2006年最新的1408395个评分，每个用户是随机分配并且最多只拥有9个评 分。Netflix将测试数据集隐藏了，测试数据集的分布和探测数据集的分布式一样的。</br></br>
###4.1预处理</br></br>
为 了在预测结果时进行预处理，我们首先将一个去全局的有待改进的方法应用到每个预测方法上，给定预测值P，如果该P的值和测试数据集的值不一样，我们将使用 一个变量T=mean(test)-mean(P)来转换所有的预测值。另外一种方法是直接将不同的预测值结合起来或获得一个更好的预测值。例如，给定两 个预测值P0和P1，我们可以获得预测值Px=(1-x)P0+xP1,并且使用线性回归（linear regression(http://en.wikipedia.org/wiki/Linear_regression)）来找到X*值来使 RMSE(Px)的值减至最小。然后我们获得的Px*值至少会比P0或P1值要好。</br></br>
  
###4.2ALS的实验结果</br></br>
在 实验当中最大的发现是ALS-WR从来不会发生数据过度拟合的情况，即使是增加循环的次数或隐藏特征值的数量都会发生过度拟合。如图1（请看原文，一定要 看！！！）所示，对于给定的nf和λ，每次循环都会提升探测数据及中的RMSE分数，并在循环20次之后该分数会趋于收敛。不同的λ值会得出不同的分数， 并且通常情况下我们更需要使用更小的λ值来得到更好的RMSE值。图2（请看原文！！！）显示了根据固定值λ和不同数量的隐藏特征值（nf值介于2到 20）获取的ALS-WR性能，对于每次试验，ALS-WR会重复循环直到RMSE值提升幅度小于1bps为止。从该图我们可以看出RMSE值会随着大的 nf值而单调递减，即使nf值是渐渐变小的（只要够大）。</br></br>
接下来我们使用大的nf值来进行试验，对于使用简单的 λ正规化（u = m = E）的ALS，我们获得的RMSE值是0.9184，对于使用权重-λ-正规化和nf为50的ALS，获得的RMS值为0.9114，如果nf值为 150，那么值为0.9066。如果nf值为300并伴有全局偏量校正，RMSE值为0.9017，这时如果nf=400，则 RMSE=0.9006；nf=500并伴有全局偏移，则RMSE=0.9000。最终，当nf=1000，RMSE=0.8985，在特征值从400增 加到500的过程中获得了6bps的提升，从500增加到1000的过程中大约有15bps的提升。然后，似乎0.8985是使用ALS-WR能够获得极 限值了，这个值比Netflix自身的CineMatch算法有了5.56%的性能提升，并且它是我们所知道的的单一方法解决此问题的最高得分了。</br></br>
###4.3其他的方法和线性混合</br></br>
我们也使用并行实现了其他两种有名的协同过滤技术。</br></br>
受 限波尔兹曼机（Restricted_Boltzmann_machine（en.wikipedia.org/wiki /Restricted_Boltzmann_machine））是一种同时存在可见状态和隐藏状态的中立图，它的无向边链接这每个可见状态和隐藏状态， 因此得名受限。该方法先前被证明在Netflix竞赛中工作的很好，我们也用Matlab实现了RBM，并将它转化为了PMODE(http://en.wikipedia.org/wiki/PMODE)。对于一个有100个隐藏单元组成的模型，在没有PMODE的情况下RBM花费了将近1小时的时间来完成一个循环，然后使用30个labs的PMODE只花费了3分钟。</br></br>
K- 近邻（KNN（http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm））算法也是一种 进行推荐的好方法。对于一个正确的距离矩阵，该矩阵中的每个数据节点都需要预测的情况下，k个最邻近值的平均得分会被用来预测当前节点的得分。由于有许多 的用户-用户对需要在合理的时间和空间内计算，我们会使用电影-电影中的相似之处来计算。我们将用户分成用户组来实现并行KNN算法，这样一个lab就只 处理一组用户了。</br></br>
对于RBM，得到的分数是0.9181，对于KNN，当k=21并且有个优秀的相似方法，得到的分数为0.9270。混合这三种方法得到的值是0.8952（ALS+KNN+RBM），同样的比Netflix自身的CineMatch推荐系统也有了5.91%的性能提升。</br></br>
 
##5相关工作</br></br>
现在有许多的学者和产业在研究推荐系统，低秩矩阵近似和Netflix prize问题。我们会简单介绍一些和我们有关系的一些相关内容。</br></br>
###5.1推荐系统</br></br>
推 荐系统可以主要分为基于内容和系统过滤两类，推荐系统在学术和产业中已经有了很不错的研究结果。基于内容的推荐系统当然是要去分析某个事物的内容（如文 本，元数据和特征）来鉴定关联事物，做的比较好的如InfoFinder,NewsWeeder。协同过滤对大量的用户行为和口味进行聚合来为指定用户推 荐相应的物品，做的比较好的有GroupLens和Bellcore Video。结合基于内容和协同过滤这两种方法的推荐系统有Fab系统和统一概率框架。</br></br>
###5.2Netflix Prize方法</br></br>
对 于Netflix Prize，Salakhutdinov et al使用受限波尔兹曼机获得了略低于0.91的RMSE分数，他们同时使用梯度下降实现了低秩近似值方法，这种方法在20-60个隐藏特征值的情况下获得 的分数略高于0.91。他们的SVD方法和我们的ALS-WR是相似的，但是我们使用的是ALS并不是梯度下降来解决优化问题，并且我们可以使用一个更大 的特征值来获得RMSE分数的明显提升。
在众多的Netflix问题的解决方案中，Bell et al结合KNN和低秩近似的方式获得分数都比使用单独一种方式获得的分数要高。他们的团队由于得到的RMSE分数为0.8712（比Netflix自身的 方法高出8.5%）在2007年10月获得了进步奖。然而他们的解决方案是将107中个性化解决方案进行线性组合，这107中方案中的大部分都是从 ALS,RBM,KNN衍生出来的。在单独使用ALS的情况下他们的最好得分是高于0.9000。</br></br>
###5.3低秩近似值方法</br></br>
我们可以使用低秩矩阵分解（奇异值分解的变体）来估算一个指定的矩阵，例如信息检索就是这样的一种情况。其他的矩阵分解方法如非负矩阵分解和最大边矩阵分解都在Netflix Prize中有所应用。</br></br>
对 于部分指定的矩阵，SVD就不适用了，为了将已知的元素和对应的低秩矩阵因子之间的平方差最小化，ALS被证明是一种有效地方式，它提供了非正交因子。 SVD会在一个时间下计算一行值，而对于部分指定的矩阵，没有这样的递归公式来解决。使用ALS的好处是它可以很容易实现并行化，像Lanczos一样， 对于稀疏并全部指定的矩阵，ALS保留了已知的矩阵元素的稀疏结果并且它的存储效率是很高的。</br></br>
 
##6.结语</br></br>
在 本文中我们介绍了一种简单的并行算法来对大数据进行协同过滤,这种算法和在文献中出版过的任何一种方法是相似的。该算法对于大数据集具有很好的可扩展性， 通过使用RBM和KNN等其他复杂的混合模式可以适当地获得更好的分数。特别地ALS-WR能够在没有日期和电影名称的情况下获得很好的结果，通过让创建 模型和调整参数这些过程并行化能够获得更快的运行速度，写一篇文章来解释为什么ALS-WR从不会过度拟合你的数据将会是一件很有趣的事情。</br></br>
由 于当今世界已经进入互联网计算和网络应用的时代，大型数据的计算变得无处不在。传统的单片机单线程计算已经变得不可行。并行和分布式计算对于大多数的计算 环境来说已经是一个必不可少的组件了。互联网老大Google公司将拥有它们自己的并行分布式计算的基础设施，这些技术如 MapReduce,Google File System,Bigtable等都做出了巨大的贡献。许多的技术型公司并没有资本和相关技术来建立自己的并行分布式计算设施，它们反而更愿意使用可行的 解决方案来解决这些问题。Hadoop是一个由Yahoo!投资的开源项目，它致力于使用开源方式来复制Google的计算设施。我们已经发现并行 Matlab是灵活和有效的，并对于程序来说是很简单的。因此，从我们的经验出发，这看起来在将来会是一个解决可扩展并行分布式计算的一个强有力的竞争 者。
